{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import time\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from itertools import cycle\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from models import build_or_load_gen_model\n",
    "from configs import add_args, set_seed, set_dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "from transformers import T5Tokenizer, T5Model\n",
    "from utils import read_review_examples, TextDataset, CommentGenDataset, CommentClsDataset\n",
    "from models import build_or_load_gen_model\n",
    "from run_finetune_cls import get_loaders\n",
    "import multiprocessing\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MyTokenizer\n",
    "tok = MyTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in tok.get_vocab().items():\n",
    "    if val == 0:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeArgs(object):\n",
    "    def __init__(self):\n",
    "        self.max_source_length = 512\n",
    "        self.max_target_length = 256\n",
    "        # self.model_type = \"scratch\"\n",
    "        self.model_type = \"t5\"\n",
    "        self.config_name = \"t5-base\"\n",
    "        self.add_lang_ids = True\n",
    "        # self.tokenizer_path = \"tokenizer\"\n",
    "        self.tokenizer_path = \"t5-base\"\n",
    "        self.model_name_or_path = \"t5-base\"\n",
    "        self.load_model_path = None\n",
    "        self.train_path = \"../../../lzzz/processed\"\n",
    "        self.train_batch_size = 1\n",
    "        self.langs = [\"ruby\"]\n",
    "        self.cpu_count = 3\n",
    "        self.num_train_epochs = 2\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.mask_rate = 0.15\n",
    "        self.from_scratch = True\n",
    "        self.local_rank = 0\n",
    "        self.global_rank = 0\n",
    "        self.world_size = 1\n",
    "\n",
    "args = FakeArgs()\n",
    "config, model, tokenizer = build_or_load_gen_model(args)\n",
    "pool = multiprocessing.Pool(args.cpu_count)\n",
    "datafiles = [\"../../../lzzz/processed/genchunk_test0.jsonl\"]\n",
    "datafiles = [\"../../../lzzz/processed/clschunk_train0.jsonl\"]\n",
    "packs = get_loaders(datafiles, args, tokenizer, pool)\n",
    "for pack in packs:\n",
    "    dataset, _, loader = pack\n",
    "    # data = next(loader)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = open(\"../../../lzzz/processed/clschunk_train0.jsonl\").readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patch': '@@ -1,0 +1,14 @@\\n+class Foo {\\n+    #tag() {\\n+      return this;\\n+    }\\n+  \\n+    get #privateTagMethod(){\\n+        return this.#tag``\\n+    }\\n+  \\n+    publicGetPrivateTagMethod(){\\n+        return this.#privateTagMethod\\n+    }\\n+  }\\n+  const instance = new Foo();\\n',\n",
       " 'y': [1,\n",
       "  '@@ -0,0 +1,14 @@\\n+class Foo {\\n+    #tag() {\\n+      return this;\\n+    }\\n+  \\n+    get #privateTagMethod(){\\n+        return this.#tag``\\n+    }\\n+  \\n+    publicGetPrivateTagMethod(){\\n+        return this.#privateTagMethod\\n+    }\\n+  }\\n+  const instance = new Foo();\\n\\\\ No newline at end of file\\n',\n",
       "  '',\n",
       "  1],\n",
       " 'oldf': '',\n",
       " 'idx': 1,\n",
       " 'project': 'review_cls_javascript_babel-babel.json',\n",
       " 'lang': 'java'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "dic = json.loads(line)\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50221, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.embed_tokens.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12791"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12795"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_src, gold_tgt = [], []\n",
    "with open(datafiles[0], \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        js = json.loads(line)\n",
    "        if \"msg\" not in js or len(js[\"msg\"]) == 0:\n",
    "            continue\n",
    "        src = js[\"patch\"].replace(\"\\n\", \"\\\\n\")\n",
    "        tgt = js[\"msg\"].replace(\"\\n\", \" \")\n",
    "        gold_src.append(src)\n",
    "        gold_tgt.append(tgt)\n",
    "len(gold_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3691\n"
     ]
    }
   ],
   "source": [
    "for i, examples in enumerate(loader):\n",
    "    if i == 1:\n",
    "        break\n",
    "    # source_ids = torch.tensor(\n",
    "    #     [ex.source_ids for ex in examples], dtype=torch.long\n",
    "    # )\n",
    "    # source_labels = torch.tensor(\n",
    "    #     [ex.source_labels for ex in examples], dtype=torch.long\n",
    "    # )\n",
    "    # target_ids = torch.tensor(\n",
    "    #     [ex.target_ids for ex in examples], dtype=torch.long\n",
    "    # )\n",
    "    for ex in examples:\n",
    "        # if ex.type == \"label\":\n",
    "        # print(f\"example type: {ex.type}\")\n",
    "        # print(f\"batch size: {len(examples)}\")\n",
    "        # print(f\"example source: {tokenizer.convert_ids_to_tokens(ex.source_ids)}\")\n",
    "        # print(f\"example label: {ex.source_labels}\")\n",
    "        # print(f\"example target: {tokenizer.convert_ids_to_tokens(ex.target_ids)}\")\n",
    "        source_str = tokenizer.decode(ex.source_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        target_str = tokenizer.decode(ex.target_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        id = ex.example_id\n",
    "        print(id)\n",
    "        open(\"modelsrc.txt\", \"w\").write(source_str)\n",
    "        open(\"modeltgt.txt\", \"w\").write(target_str)\n",
    "        open(\"examplesrc.txt\", \"w\").write(gold_src[id].replace(\"\\\\n\", \"\\n\"))\n",
    "        open(\"exampletgt.txt\", \"w\").write(gold_tgt[id])\n",
    "    # source_mask = source_ids.ne(tokenizer.pattd_id)\n",
    "    # target_mask = target_ids.ne(tokenizer.pad_id)\n",
    "    # d = model(\n",
    "    #     input_ids=source_ids,\n",
    "    #     input_labels=source_labels,\n",
    "    #     decoder_input_ids=target_ids,\n",
    "    #     attention_mask=source_mask,\n",
    "    #     decoder_attention_mask=target_mask,\n",
    "    # )\n",
    "    # print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(\"testo\")\n",
    "tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data[0].source_ids\n",
    "labels = data[0].source_labels\n",
    "targets = data[0].target_ids\n",
    "# print(\"\".join())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyprint(inputs):\n",
    "    for i, tok in enumerate(inputs):\n",
    "        if i > 0 and str(tok).startswith(\"<e\"):\n",
    "            print(\"\")\n",
    "        print(str(tok) + \" \", end=\"\")\n",
    "toks = tokenizer.convert_ids_to_tokens(inputs)\n",
    "labs = labels\n",
    "tgts = tokenizer.convert_ids_to_tokens(targets)\n",
    "# print(toks)\n",
    "prettyprint(toks)\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "prettyprint(labs)\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "prettyprint(tgts)\n",
    "print(\"\\n\\n\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(dataset.examples[0].prevlines))\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "print(\"\\n\".join(dataset.examples[0].lines))\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "print(\"\\n\".join(dataset.examples[0].afterlines))\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "print(dataset.examples[0].msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "idx = 0\n",
    "with open(\"../../../lzzz/processed/ruby_cls.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        js = json.loads(line)\n",
    "        data.append(js)\n",
    "        # print(js[\"patch\"])\n",
    "        idx += 1\n",
    "        if idx == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import json\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.encode(\"ff f\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = ByteLevelBPETokenizer(\"tokenizer/vocab.json\", \"tokenizer/merges.txt\")\n",
    "tok.encode(\"+\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ReviewExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ReviewExample(0, open(\"oldf.txt\").read(), open(\"diff.txt\").read(), \"it does not matter.\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.labels[105:120]\n",
    "exp.msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.input.split(\"<e0>\")[105:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "path = \"../../../lzzz/processed\"\n",
    "files = os.listdir(path)\n",
    "files = [os.path.join(path, f) for f in files if f.startswith(\"genchunk_train\") and f.endswith(\".jsonl\")]\n",
    "# files = files[:1]\n",
    "allcnt, cnt, cnt2, cnt3 = 0, 0, 0, 0\n",
    "for file in files:\n",
    "    f = open(file, \"r\")\n",
    "    for line in f:\n",
    "        js = json.loads(line)\n",
    "        if \"```suggestion\" in js[\"msg\"][:15]:\n",
    "            # open(\"oldf.txt\", \"w\").write(js[\"oldf\"])\n",
    "            # open(\"diff.txt\", \"w\").write(js[\"patch\"])\n",
    "            # open(\"msg.txt\", \"w\").write(js[\"msg\"])\n",
    "            cnt += 1\n",
    "        if js[\"msg\"].startswith(\"```suggestion\"):\n",
    "            cnt2 += 1\n",
    "        if \"suggestion\" in js[\"msg\"]:\n",
    "            cnt3 += 1\n",
    "        allcnt += 1\n",
    "    f.close()\n",
    "print(allcnt, cnt, cnt2, cnt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = json.loads(f.readline())\n",
    "open(\"oldf.txt\", \"w\").write(js[\"oldf\"])\n",
    "open(\"diff.txt\", \"w\").write(js[\"patch\"])\n",
    "open(\"msg.txt\", \"w\").write(js[\"msg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js[\"msg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_review_examples, TextDataset, MyTokenizer\n",
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(cpu_count)\n",
    "from transformers import T5Tokenizer\n",
    "class FakeArgs:\n",
    "    def __init__(self):\n",
    "        self.max_source_length = 512\n",
    "        self.max_target_length = 256\n",
    "args = FakeArgs()\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "tokenizer = MyTokenizer.from_pretrained(\"tokenizer\")\n",
    "class DebugDataset(TextDataset):\n",
    "    def __init__(self, tokenizer, pool, args, file_path, samplenum=-1):\n",
    "        self.cnt = 0\n",
    "        savep = file_path.replace(\".jsonl\", \".exps\")\n",
    "        examples = read_review_examples(file_path, samplenum)\n",
    "        self.examples = pool.map(self.tokenize, \\\n",
    "            [(example, tokenizer, args) for example in examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = read_review_examples(file_path, samplenum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.871458557455923\n"
     ]
    }
   ],
   "source": [
    "v = []\n",
    "for i, ex in enumerate(examples):\n",
    "    line = ex.input\n",
    "    f = tokenizer.encode(line)\n",
    "    l1 = len(f) - 1\n",
    "    lines = line.split(\"<e0>\")\n",
    "    lines = [l.split() for l in lines]\n",
    "    l2 = sum(map(len, lines))\n",
    "    v.append(l1 / l2)\n",
    "print(sum(v) / len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"\": [\"<e0>\"]})\n",
    "tokenizer.special_dict = {\"</s>\": tokenizer.get_vocab()[\"<e0>\"]}\n",
    "file_path = \"/home/v-zhuoli1/lzzz/processed/chunk_25.jsonl\"\n",
    "samplenum = 1000\n",
    "d = DebugDataset(tokenizer, pool, args, file_path, samplenum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ex in enumerate(d.examples):\n",
    "    labels = ex.labels\n",
    "    if sum(labels) == len(labels) * -100:\n",
    "        print(i, ex.msg)\n",
    "        print(ex.lines)\n",
    "        print(ex.labels)\n",
    "        # break\n",
    "print(len(d.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example nums: 101937\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "path = \"/home/v-zhuoli1/lzzz/processed/genchunk_train0.jsonl\"\n",
    "msgs = []\n",
    "with open(path, \"r\") as f:\n",
    "    for line in f:\n",
    "        js = json.loads(line)\n",
    "        msgs.append(js[\"msg\"])\n",
    "print(f\"Example nums: {len(msgs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "smsgs = random.sample(msgs, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just noticed there are two whitespaces after the `?` :smile: ',\n",
       " \"We chatted a bit about this in DM's, would you mind summarizing the result of our conversation (RE: why this is fine to remove)?\",\n",
       " \"Can I suggest you to add more debug logs on key places? It was really hard for me to find cassandra.ensureSchema was really set to false, I thought it could be an error to set the variable... I think it's worth logging it as well, as debug  Not on this PR, but I think it would be great as well to have a command line option on zipkin server to dump all spring config. It's not something hard to do, but extremely helpful for the user.\",\n",
       " \"I skimmed this and didn't immediately see the syntax error, can you just add a comment explaining it? Thanks!\",\n",
       " \"`dask_array_type` is actually a tuple, so this needs to be `(Iterable,) + dask_array_type`.  That said... this already tricky logic is getting even more convoluted. So this isn't my favorite fix, though I agree that an N-dimensional dask array should not be considered a scalar.\",\n",
       " 'This will break Bazel; use a literal `2`.  The comment alludes to why this was done, but even if you thought that no longer applied the comment should have been removed.',\n",
       " \"As a best practice, it's better to not use a negation in an if/else statement. i.e. write:  ``` if (foo === bar) {   // do something if true } else {   // do something if not true } ```  And more confusing to write: ``` if (foo !== bar) {   // do something if not true } else {   // do something if true } ```\",\n",
       " 'Should really be `np.core.multiarray` instead',\n",
       " \"isn't this already set?\",\n",
       " 'Removing this method is a BC break. @Ocramius is it acceptable? If so it has to be documented in the `UPGRADE.md`. ',\n",
       " \"AFAIK this won't compile on the recent master \",\n",
       " '```suggestion     // TODO update getAddons to ```',\n",
       " 'See comment above about including the possible solution in the deprecation message',\n",
       " \"I'm a little confused about what setting `options.promiseLibrary` before running `executeLegacyOperation` does. As you can see  , `executeLegacyOperation` ignores the `options` and grabs the `promiseLibrary` from the `topology`. If I'm not missing something, we can just delete anything that sets `options.promiseLibrary` before a call to `executeLegacyOperation`.\",\n",
       " 'We should think about moving all these tests away from `unittest.TestCase` if possible, there are some really cool pytest features that we are missing out',\n",
       " '@Scottmitch finalizers make me  ',\n",
       " '@mlocati IMO, this should be `@since 8.2.1`',\n",
       " 'Can we change this line to `type: Boolean` instead?  ',\n",
       " 'Small nit but unfortunately the plural is just \"middleware\"; shall we call this \"stylisPlugins\" instead?',\n",
       " 'String comparison should be `===` strict. ',\n",
       " 'Missing a Summary ',\n",
       " \"Return false?? Geez this file is messy. Would you mind cleaning up these messages so they're accurate?\",\n",
       " 'Could `last` actually be null in some situations? Might be safer to do a nullthrows',\n",
       " \"need the namespace delimiter at the end of your prefix filter, otherwise you'll over-match  e.g. deleting usage for the namespace 'foo' would also end up matching and deleting for the namespace 'foobar'. Making it 't.foo:' should be good enough.\",\n",
       " 'Can we encapsulate these prop readings into `PartialDisconnectionHandler` to increase isolation?',\n",
       " 'This should be `7` now.',\n",
       " 'This message needs to be updated.',\n",
       " \"Let's do  ```python         blacklisted_files = instance.get('blacklist_conntrack_metrics')         whitelisted_files = instance.get('whitelist_conntrack_metrics')         if blacklisted_files is None and whitelisted_files is None:             whitelisted_files = ['max', 'count'] ``` It's clearer what we are doing, and we need to be able to specify an empty whitelist to override the default\",\n",
       " 'Why are the warnings cleared here?',\n",
       " 'is this `|| false` expression redundant?',\n",
       " \"Don't think this needs to be an `else if`, is that intentional?  It needs to be in focus every time it is newly opened IMO. If achieving that behavior requires a standalone react effect, that's fine as well.\",\n",
       " 'Could we just use `__VERSION__` for this?  Looking at the output of: ``` echo \"\" | gcc -dM -E - &> gcc echo \"\" | clang -dM -E - &> clang echo \"\" | gcc-7 -dM -E -v - &> gcc7 ```  clang and gcc are the same thing (the output is identical).  `__VERSION__` and `__clang_version__` would work for a \"vendor\" string.  gcc-7 is from homebrew, but `__VERSION__` is just the version of GCC.  I think this means we\\'d treat any GCC 7.2.0\\'s as the same thing.  I\\'d like to see some more examples of the ARM compilers we know about and if we can rely on `__VERSION__` because I think that\\'s going to be good enough for now.',\n",
       " \"don't remove. look at the corresponding assertion events to be added to this.\",\n",
       " 'Maybe yoda?',\n",
       " 'Write a comment about this sorting as well.. ',\n",
       " 'Should this to set to false at the end of the method? It seems a little inconsistent with the way `#initialize` sets it to false to leave it true after this method; otherwise, it\\'s possible for it to be both \"connecting\" and \"connected\", which seems incorrect.',\n",
       " 'FWIW, `json.dumps` also has an option to sort keys during encoding.',\n",
       " 'Looks good to merge but maybe we should here also execute `self:: isLookLikeUrl ($url)` and only return true if it actually is a url?',\n",
       " '```suggestion         assert !(current instanceof QueryDataSource); // lgtm [java/contradictory-type-checks]                  current = inlineIfNecessary(current, null, subqueryRowLimitAccumulator, maxSubqueryRows, dryRun); ```',\n",
       " 'We could include the configured limit in the \"reason\" wherever we use this error to aid troubleshooting',\n",
       " 'Add a TODO(#4535) as above',\n",
       " 'Please consider using `src/researches/findKeywordFormsInString.js` for consistency.',\n",
       " '`isDescriptor` is inherited now ',\n",
       " \"If this call was wrapped in `deprecated()` earlier shouldn't the method be removed all together in 4.x?\",\n",
       " 'can you fix indentation ? ',\n",
       " 'Is there a reason this needs to be a function? Looks like it could be a simple constant now. Of course, that will add a few more changed lines, but ultimately it will make the code simpler, I think.',\n",
       " 'Is it possible to track the call stacks for the leaks in question?',\n",
       " \"Should this be in the guild object if discord doesn't provide them in the guild object?\",\n",
       " 'Nit: Maybe `showRemoveConfirmation` to be more explicit?',\n",
       " 'if only two things was to get parameterized it should be the logging level and the format string ',\n",
       " 'probably we can remove the repeat rule. not important though ',\n",
       " 'is there a test for the multi-agent preprocessing?',\n",
       " 'Need phpdoc.',\n",
       " \"This shouldn't be necessary, in theory; the generated wrapper should unwrap the signal.\",\n",
       " 'Use `_` here, and below. ',\n",
       " 'Change \"Smpp\" to \"SMPP\" here. Camel-case naming convention applies to Java code, but log messages should follow English language rules. SMPP is an abbreviation, so should be all upper case, same as SMS.',\n",
       " 'Should this go in `google.cloud._helpers` next to `_RFC3339_NO_FRACTION`?',\n",
       " \"We don't need to explicitly stop the readers as before?\",\n",
       " 'dead code',\n",
       " 'Have you checked this on Android?  It creates a bug when typing in a TextField',\n",
       " 'Nit: Consider formatting this to one per line.',\n",
       " \"Shouldn't this have a `return`? \",\n",
       " 'Unncessary. remove this',\n",
       " 'Would it be worth wrapping this and the conditionals above into `BlobHelper#include_in_languages_stats?` or something like that?  ``` ruby module BlobHelper   def include_in_language_stats?     !vendored? &&     !documentation? &&     !generated? &&     language && DETECTABLE_TYPES.include?(language.type)   end end ``` ',\n",
       " 'As a style convention, do we prefer the ternary pattern or `&&` chain pattern, like:  ```js {componentsX && small && (   <circle /> )} ```',\n",
       " 'It seems character encoding is not right.  ',\n",
       " 'Feedback from `core/helpers/data` also needs to be applied here',\n",
       " 'Heads up, you can override the configured gemini window size like so.',\n",
       " 'Static import `StorageClass.Standard`.',\n",
       " \"I suggest we could live without this type. One less abstraction cost of calling `properties == null ? null : properties.getProperty(key);` in all implementations.  If I'm not missing anything, this far we only have 1 implementation anyway.\",\n",
       " 'Should we push this down to TI? Somewhere around here:   Though currently, only SubDagOperator overwrites the `pre_execute` hook, I think the `pre_execute` hook is still a public API and users may override it when they build custom operators.',\n",
       " 'I havent tested yet but im pretty sure this wont actually stop storages with files getting deleted.  FetchAll with use pdo::fetch_both by default which would be like ``` [ [0]=>[fID]=>1, [1]=>[fID]=>2, ] ```  You have a hasFiles function now use it also, if the default storage has 0 files should you be able to delete it? Even though it has 0 files in the database it still contains all the cache/generated thumbnails/avatars.',\n",
       " \"Notice here the test doesn't need to change at all (initial profile tailoring is used, if any) if the `os_minor_version` attribute is missing from the API response (COMP-E-133 feature disabled in the backend). I also added a check for `'tailored': False` for good measure.\",\n",
       " '`await this.driver().loadExtension(path);` should work',\n",
       " 'I tend to prefer explicit over implicit',\n",
       " 'What about loops? Previously we have treated them separately. ',\n",
       " 'This buffer is too small. I suggest 4/8/16MB or so.',\n",
       " 'Does this function set lineage/inlets/outlets too? If not it should please.',\n",
       " 'Nit: We should be consistent about quotes. ',\n",
       " 'i am not sure if this is correct.  say we have etcd nodes: a, b, c  This watch watchs on node a. if a loses its leader, the leader notify would trigger. it might cancel watchers on node b and c, which actually have leader. ',\n",
       " 'Even if this might be correct it should only be changed by a merge when we actually make a new release, I guess',\n",
       " '```suggestion       output += `${index + 1}/${array.length} ${item.name}\\\\n`; ```',\n",
       " 'remove this debug line',\n",
       " 'it is to exclude `./index.js`?  seems it should not be matched in the given files.',\n",
       " \"`[???]` should be `[DEBUG]` or similar, if you'd like to keep this line \",\n",
       " 'This whole code is very very very ugly, and not just this if body....',\n",
       " \"Right now, this unit test is the only place we're using this function since creating/deleting cloud variable functionality doesn't exist yet.\",\n",
       " \"I'm not that familiar with Flow, what's the driving factor for checking `isFlow` in this case? \",\n",
       " 'not for this line per se, but assuming you plan to add tests for the new feature?',\n",
       " 'Missing space before \"type\"?',\n",
       " 'If there is some org without any notification channels yet, then no default route will be added to that org here. But I think we need at least 1 default routing policy (and a contact point to attach to that) per org. If we have a list of all orgs, then it would be easiest to iterate through that list here instead of `allChannels` (which will end up adding an empty contact point and default routing policy for such orgs).',\n",
       " 'Intentional removal of this duplicate update of dateTimeStamp ',\n",
       " 'Could you implement a unit test for this method (that now implements a more complex logic)? ',\n",
       " 'Sorry if I am wrong but, Is this sentence right?  AFAI understand, the request could have no AMI but only filters, right? So it would be something like `\"Getting images with request: \" + request`?',\n",
       " 'nothing is done with the element found?',\n",
       " 'The comments in the test cases are not quite informative. Consider either adding more details or removing redundant ones?',\n",
       " 'Remove extra space.',\n",
       " 'Delete this',\n",
       " 'Make that as a class contant?',\n",
       " 'May I suggest StringUtils.isNotBlank(roleType) here? It checks for null and empty cases',\n",
       " 'This, together with the change of the API of `root.write` is needed, so that we get a more useful log line number at: https://github.com/ethersphere/go-ethereum/blob/swarm-network-rewrite/swarm/api/http/error.go#L148',\n",
       " 'This test fails because the Fixture does not contain a <del> tag.',\n",
       " 'Looks good. Do we have negative test cases, like invalid table name or alias for the join order? ',\n",
       " 'Do you think we can have a test case for your claim? > If we still directly copy and overwrite it, the configuration will be a wrong config.',\n",
       " '@ravening can you please make both variables private?',\n",
       " \"minor: I'd imagine dates will be use in other places so should probably be in `utils`?\",\n",
       " 'Please remove this commented-out line.',\n",
       " 'why allow overriding?',\n",
       " 'With timeout as seconds, we could still support sub-seconds by using float number. ',\n",
       " 'Why is this and `preview_layout` needed? Overriding? ',\n",
       " \"```suggestion     with docker_run(compose_file, log_patterns='Server startup complete', env_vars=env, sleep=5): ```\",\n",
       " 'Want to keep it 10s instead of 100s? :)',\n",
       " 'Missing @LogMessage annotation, is this intentional? ',\n",
       " \"there's no `consentString` in the uspapi\",\n",
       " 'Why not have it default to SIGINT',\n",
       " 'isn\\'t this \"hostname -f\"?',\n",
       " 'remove this annotation',\n",
       " 'Why it is important for this to be an OrderedDict? Does the order of the ids matter? Perhaps a comment is in order. ',\n",
       " 'I guess that was when you used to log different errors, but as it stands, there is no need for a `if`.',\n",
       " '@cheese8 Can it be replaced by the `isContainsJoinQuery` method?',\n",
       " 'Does this change the way non-spellcheck lint behaves? We also probably want to make it this delay configurable via a e.g. a UI pref (no need to add actual UI for the pref at this point)',\n",
       " \"I'd prefer not drying up the `contextTypes`  1. it is clearer to read the types 2. it is possible we'll have different context types for each component, i.e. does this use shortcuts?\",\n",
       " \"this outer for loop seem pretty useless to me now? can you if it's still of use at least document what it does? I looked briefly and it seems like we exit with a break right away? I think it was only there to overcome the CME that was caught here\",\n",
       " 'nit: update comment',\n",
       " \"This is totally unrelated to the rest of the PR, but I happened to notice it while working on this PR, and it didn't justify a separate PR. Without the backticks the underscores are rendered as markdown, so this renders as <b>init</b>.py ...\",\n",
       " '`Withdraw on` -> `Withdraw from`',\n",
       " '`fv.version` (in string formatting line) will currently give you **only** the version number substring (i.e. `Version X.XXX`).  Recommend changing the `fv.version` here to `fv.get_version_string()`.  The method returns full semicolon delimited version string with all included metadata. ',\n",
       " 'It occured to me that one could sum the `\"amount\"` of the outputs of the election to get the `total_votes` and avoid the query.',\n",
       " 'Missing space after the prior here, also some extra spaces after `The`. ',\n",
       " 'Please use Singleton annotation and remove this explicit binding. We plan to remove existing such usage pattern.',\n",
       " \"I think you can remove the value from the yield, it's duplicated in the instance fixture otherwise.\",\n",
       " 'I think this can be confused if we want to resolve User by ParamConverter ? ',\n",
       " \"I'm finding this doco a little hard to follow.  It would shed some light on the function's purpose if the comment were structured a bit differently.  Perhaps along these lines:  ```go // addVoteIndex registers a vote in ... for the purposes of ... .  The height and spend  // status of the vote are used to uniquely assign an index ... .   ```  The details of how the map is used and what happens if the vote height-spend combo does/doesn't exist yet are implementation details that could go as line comments within the function.  For the function documentation, something to describe the purpose and semantics are more important.  Admittedly, this might involve describing the structure of the map, unless this is the only function that writes to it, in which case the user is probably not concerned with what it does to the map, just what information it provides and stores.\",\n",
       " 'a bit late to the party :) excellent PR overall. will mention a few things that I noticed reviewing that will might help later somehow.  this function definitely wants some split I would say... extract presentation logic out for example, and the that checks dir and excludes it? or may be some other refactoring ... but it\\'s too long and complicated now  overall \"internal-client\" functions should be easy to read, even if it feels that extraction does not make much sense (e.g. helpers that are used only in a single place) think how will developers read this. It\\'s easier to read story-like main function and go into details if needed',\n",
       " 'These doc strings should be corrected.',\n",
       " \"don't we need to encode this too?\",\n",
       " 'Should we also rename `this.$lastColor` and  the `.last_color` CSS class? ',\n",
       " \"I'm not sure why this change is needed?\",\n",
       " \"I think you don't need here the application context since you can use the activity context, so just `getPackageName()` should work\",\n",
       " 'or execution id?',\n",
       " 'Nit: `swg.StopAndWait()`?',\n",
       " \"I left the route here as it was previously defined, but wanted to highlight that the Legacy Service supported both `issues` and `pulls` (`github/issues|pulls/detail`). Is the desire to narrow the official path to just be `issues`?  If so I'll add a redirector service to prevent breaking anyone using `github/pulls/detail/...`\",\n",
       " 'You can remove this line now.',\n",
       " 'comment is good, mean add `reason=...` in the xfail itself',\n",
       " \"nit: Why don't we merge these 2 tests into one file?  Structure of the tests are similar and things you test here are highly related. \",\n",
       " '`routingPoints` could be inlined there as well.',\n",
       " 'Could simply return `false` here, and eliminate the local variable `dotOrDotDotFound`.',\n",
       " 'it\\'s better to specify the time unit in the string message. for example: `\"time:%fms, loops:%d, rows:%d\"`',\n",
       " 'This should query across Shadow DOM boundaries, as aria-hidden does cascade in there. Demo to check: https://codepen.io/marcysutton/pen/bQGpEy?editors=1010',\n",
       " \":x: Should create an immutable copy here, or `ComponentVariant` will not honor its contract of being immutable. Possibly, since it's an internal class, the field type can be `ImmutableList<Exclude>`.\",\n",
       " \"Do you ever use this `input` method passing in the `styles` argument? I see one usage above, but it's not using that arg.\",\n",
       " 'I think the versioning scheme for the configs / serializers we tend to use starts from version 1. Should we stick with that?',\n",
       " 'I added this option early in the work on this patch, before refining how much I was biting off. I will add tests for it in the next bit of work; it is not yet used.',\n",
       " 'did u mean to put this return statement inside try-catch? ',\n",
       " \"Technically, this is a lie. isort currently works when run with Python 2.7.  But, it doesn't matter what Python version you run isort with, unlike `flake8` and `bandit`. We can be confident that the user has Python 3.6+ on their machine because that's how they're running Pants. So, we can safely set the constraints to Python 3.6+.  (Also, the upcoming isort 5.0 will have this same constraint.)\",\n",
       " 'Technically we fail _all_ RPCs instantly if we are in `SHUTDOWN`.  Maybe just mention `TRANSIENT_FAILURE` here?  ',\n",
       " 'Java driver too',\n",
       " 'Does it make sense to initialize the array with a capacity based on the number of commas in the strings? Not sure this matters much though. Probably best to keep the code simpler :) ',\n",
       " \"Shouldn't this be `'signup/onboarding-flow'`? :D\",\n",
       " 'support of DateTimeInterface is needed here as well ',\n",
       " 'Is there a JIRA created to remove exceptionPayload? ',\n",
       " '`getProperty.ifPresent(property -> if (property.hasExlusion){ .... } )` ',\n",
       " 'It might be a bit clearer if ZipPath define isSelfOrParent(), that would avoid needing to expose the internal representation.',\n",
       " 'This definition should be moved into the module. Can you try a shim like this:  ``` window.FileError = window.FileError || {     ... }; ``` ',\n",
       " 'I think the fix is in the wrong place. Member::default_admin() should NOT be returning null if a default admin is specified. that method is meant to be guaranteed to return an instance. Can you please investigate why this is not happening?',\n",
       " 'I actually think we shouldn\\'t return the underlying server here, and instead we should change the `GetTopoServer()` signature to return `(*topo.Server, error)` and return an error of \"Cannot modify underlying topology server when keyspace filtering is enabled\" here.  The reason for this is that `GetTopoServer()` is only needed as part of the vschema DDL management implementation at vtgate (see and not as part of the general query serving path.  The vschema ddl implementation assumes that the SrvVschema that it is operating on is complete, and as such when vschema DDL operations make changes to the vschema, it writes back the updated SrvVschema to each cell.  When used with keyspace filtering, this would have the very undesirable effect of removing the filtered-out keyspaces from the SrvVschema globally.  So, I think it\\'s only safe to allow vschema DDL operations when keyspace filtering is not in use, and that the best way to enforce that is to hide the underlying topo server altogether here.',\n",
       " 'This method is copied verbatim from the GridBundle. Not really sure how to avoid that without adding a new shared dependency just for this section of code.. ',\n",
       " 'missing space after copyright sign ',\n",
       " \"BTW, this was also a bug. You should pass in the  `ChildHelper` instance of the `LocalityLbInfo` instead of just `helper`. It's lucky that `ChildHelper`'s `SynchronizationContext` is just a delegation to `helper`'s, so it did not cause problem. But this is wrong.  Argument-passing, object ownership, lifecycle, etc. A lot of things in `LocalityStore` are messy. I hate this class, really. It might still contain unnoticed bug. Hope we can refactor to the LB structure and get rid of this early.\",\n",
       " 'please add here `->once()` or something similar',\n",
       " 'Should it mention PostgreSQL?',\n",
       " \"I couldn't find a typesafe way to express that binaryNode has `AppleTestDescription.Arg`, but after the typecheck this should be true. Suggestions welcome. \",\n",
       " 'Should this not happen outside of the loop, and the loop then moved into the else block on this condition? Edit: on second thought that would open Unindexed fields, so maybe not :) ',\n",
       " 'this is some additional way to run tests? What is the difference between passing CI=true ?',\n",
       " 'Does this need to be `e+2`?',\n",
       " 'Do any of these need cleaning up?',\n",
       " 'Instead of modifying webpack config, could we define the feature flags in the scss file that we have?',\n",
       " 'Which method call throws this exception?  If Volt is not ready, it may also throw Exception from line 215? ',\n",
       " 'Style comment, curly braces should be on a new line. ',\n",
       " 'same comment as in #4033: first we suspend unconditionaly, and than, we just assume it need resuming without checking if it was running in the first place. Are there situations we should guard against more diligently? (cc @weizhouapache @rhtyd )',\n",
       " 'so what I meant originally, is you can add a ``_validate_freeze_panes`` method HERE (to avoid duplicating this code)',\n",
       " \"```suggestion             ->setParameter(':fieldId', $field->id, ParameterType::INTEGER) ```\",\n",
       " \"Are you sure this is gonna resolve correctly within AS7's modular classloading architecture?   The reason I ask is because AS7 might itself deploy infinispan-core, but infinispan's query module might be deployed in a user deployment, and this call assumes that both are in the same classloader because it uses ConfigurationValidatingVisitor's classloader to resolve the Class.forName() call.  What I'm trying to get to is whether you should use the configured classloader (Configuration.getClassLoader()) rather than assume the same classloader here. \",\n",
       " 'Test this by executing a SQL with both sequence and normal table. e.g. `select seq.nextval(), t.id from t`.',\n",
       " \"Actually now that this doesn't use any `notify` mechanism, can be replaced with the use of `Awaitility`. So blocked by to add Awaitility dependency.\",\n",
       " \"i'd perhaps create a lookup function mapping designType to a default themeSlugWithRepo but don't understand why simulating submission of the themes step is needed instead of just providing the themeSlugWithRepo dependency from the design-type step.\",\n",
       " 'Is +1 required here?',\n",
       " 'As @sohkai points out, the `networkId` that is used to generate the snapshot is `15`. We should keep using 15 to avoid potential issues.',\n",
       " 'Can you please add a comment something along the lines of \"Static strings in this class should be considered \\'reserved words\\' for the `context` of a query.\"  We\\'ll slowly move over the other reserved words to this class. ',\n",
       " 'this indentation looks wonky ',\n",
       " 'Typo: principal ',\n",
       " 'This looks like something for `TypeSerializerUtils`.',\n",
       " '```suggestion       ![this.guild.rulesChannelID, this.guild.publicUpdatesChannelID].includes(this.id)) ```',\n",
       " 'Just use a DefaultChannelGroup ? ',\n",
       " 'It is better to rename `Fake` as `Fixture`, just keep consist with original codes',\n",
       " 'assert the return value. ',\n",
       " \"I haven't checked how this shows up in the documentation, but I think it is overkill to mark it deprecated (especially if it shows up in an obnoxious way on every observable in the docs).  It was just over a month ago that @elemoine agreed (see \",\n",
       " 'can you use `match`? something like `match=\"Warmup samples\"` should be enough, it\\'s only to prevent the test from passing if there is some unrelated warning.',\n",
       " 'This was \"fun\" to track down. Without the read, the pexpect.spawn instance was getting garbage collected and terminating the subprocess somehow before its `atexit` handlers had a chance to run and record coverage data.',\n",
       " 'See above regarding $hasEvaluated initialization ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsgs = [s.replace(\"\\n\", \" \") for s in smsgs]\n",
    "smsgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf = \"../../../lzzz/processed/python_gen.jsonl\"\n",
    "dics = []\n",
    "with open(dataf, \"r\") as f:\n",
    "    for line in f:\n",
    "        js = json.loads(line)\n",
    "        dics.append(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3441\n"
     ]
    }
   ],
   "source": [
    "def countpro(s, dics):\n",
    "    cnt = 0\n",
    "    for dic in dics:\n",
    "        if s in dic[\"project\"]:\n",
    "            cnt += 1\n",
    "    print(cnt)\n",
    "countpro(\"pandas-dev-pandas\", dics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import random\n",
    "\n",
    "dataf = \"../../../lzzz/reviewdata\"\n",
    "files = os.listdir(dataf)\n",
    "cls_files = [os.path.join(dataf, f) for f in files if f.startswith(\"review_cls\") and f.endswith(\".jsonl\")]\n",
    "gen_files = [os.path.join(dataf, f) for f in files if f.startswith(\"review_gen\") and f.endswith(\".jsonl\")]\n",
    "cls_file = random.choice(cls_files)\n",
    "gen_file = random.choice(gen_files)\n",
    "\n",
    "def read_jsonl(file):\n",
    "    dics = []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            js = json.loads(line)\n",
    "            dics.append(js)\n",
    "    return dics\n",
    "# cls_data = read_jsonl(cls_file)\n",
    "gen_data = read_jsonl(gen_file)\n",
    "# random.shuffle(cls_data)\n",
    "# random.shuffle(gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_map = {}\n",
    "cnt = 0\n",
    "for dic in gen_data:\n",
    "    msg = dic[\"msg\"]\n",
    "    if msg not in msg_map:\n",
    "        msg_map[msg] = dic\n",
    "    else:\n",
    "        print(msg + \"\\n\")\n",
    "        print(msg_map[msg])\n",
    "        print(dic)\n",
    "        cnt += 1\n",
    "        break\n",
    "print(cnt)\n",
    "print(len(gen_data))\n",
    "print(cnt / len(gen_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "dic = cls_data[i]\n",
    "open(\"patch.txt\", \"w\").write(dic[\"patch\"])\n",
    "print(dic[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7673"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i += 1\n",
    "dic = gen_data[i]\n",
    "open(\"patch.txt\", \"w\").write(dic[\"patch\"] + \"\\n\\n\\n\" + dic[\"msg\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9284e13026beb4ed65c9bcdab837c39250db4314eb9c8e651b6ffb0b51a50d53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
